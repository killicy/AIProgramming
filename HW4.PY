# -*- coding: utf-8 -*-
"""HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VEG-j9XgggUSRCnE6Xec0kV1xo1rtSYY

#Activation Functions

##Softmax

Takes as input a vector of *k* such that all numbers are real, and normalizes it into a probability distribution consisting of *k* probabilities proportional to the exponentials of the initial input numbers
"""

import numpy as np

mmatrix = np.array([[1,2,3],[4,5,6]])

def softmax(X):
    expo = np.exp(X)
    expo_sum = np.sum(np.exp(X))
    return expo/expo_sum

print (softmax(mmatrix))

"""##ReLu

An activation function that uses only the positive real numbers in it's argument. This allows for sparce activation, and better gradiant propagation, and accelerates the training speed of deep neural networks compared to sigmoid activation functions.
"""

import numpy as np

mmatrix = np.array([[1,2,3],[4,5,6]])

def relu(X):
   return np.maximum(0,X)

print (relu(mmatrix))

"""##Sigmoid

Extremely simliar to softmax, but Sigmoid is used in binary classification while the Softmax is used for multi-class tasks.
"""

import numpy as np

mmatrix = np.array([[1,2,3],[4,5,6]])

def sigmoid(X):
   return 1/(1+np.exp(-X))

print(sigmoid(mmatrix))

"""#Training/Loss/Descent

##Linear Regression

The loss up and down on a linear function.

##Gradient Descent

Optimization algorithm that finds the minimum of a function
"""

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits import mplot3d
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
import numpy as np
from numpy import ma

def f(x, y):
    return x*x + y*y

x = np.linspace(-6, 6, 30)
y = np.linspace(-6, 6, 30)

X, Y = np.meshgrid(x, y)
Z = f(X, Y)
U = X
V = Z

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.contour3D(X, Y, Z, 150, cmap='RdGy')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z');

ax.scatter(xs=0, ys=0, zs=-1, zdir='z',)
ax.scatter(xs=0, ys=0, zs=0, zdir='z')

"""##Stochastic Gradient Descent

An optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the back-propagation of errors algorithm

##Learning Rate

The rate at which a nueral network learns, and descends a gradient. It must be modified properly, since it can easily be to large or too small a number. Too large you will miss the minimum, and too small you will do far too many calculations.

#Model Terminology

##Label

The thing the model is trying to predict. Such as the y or x in a function. The model's job is to get as close as possible to correctly guessing/predicting the label, and is also used to assist in training.

##Feature

The variables that are input into the model. It can be as simple as x for a linear function, or have many multi-parts to models that take large amounts of input.
"""
